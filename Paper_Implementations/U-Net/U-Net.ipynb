{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732f7b97",
   "metadata": {},
   "source": [
    "# U-Net Implementation From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa780399",
   "metadata": {},
   "source": [
    "Here I'm going to be using a different biomedical dataset because I couldn't download the one used in the original from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf47610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eef06",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder / Contracting Path\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x) # The skip connection\n",
    "        out = self.pool(feat) # The input for the next layer\n",
    "        return feat, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a86666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(enc_feat, target_tensor):\n",
    "    _, _, h, w = target_tensor.shape\n",
    "    enc_h, enc_w = enc_feat.shape[2], enc_feat.shape[3]\n",
    "\n",
    "    delta_h = enc_h - h\n",
    "    delta_w = enc_w - w\n",
    "\n",
    "    return enc_feat[:, :, delta_h // 2 : enc_h - delta_h // 2, delta_w // 2 : enc_w - delta_w // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder / Expanding Path\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, enc_feat):\n",
    "        x = self.upconv(x)\n",
    "        enc_feat = center_crop(enc_feat, x)\n",
    "        x = torch.cat([enc_feat, x], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = Encoder(in_channels, 64)\n",
    "        self.enc2 = Encoder(64, 128)\n",
    "        self.enc3 = Encoder(128, 256)\n",
    "        self.enc4 = Encoder(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = Bottleneck()\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = Decoder(1024, 512)\n",
    "        self.dec3 = Decoder(512, 256)\n",
    "        self.dec2 = Decoder(256, 128)\n",
    "        self.dec1 = Decoder(128, 64)\n",
    "\n",
    "        # Final Conv 1x1\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1, p1 = self.enc1(x)\n",
    "        s2, p2 = self.enc2(p1)\n",
    "        s3, p3 = self.enc3(p2)\n",
    "        s4, p4 = self.enc4(p3)\n",
    "\n",
    "        b = self.bottleneck(p4)\n",
    "\n",
    "        d4 = self.dec4(b, s4)\n",
    "        d3 = self.dec3(d4, s3)\n",
    "        d2 = self.dec2(d3, s2)\n",
    "        d1 = self.dec1(d2, s1)\n",
    "\n",
    "        return self.final(d1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075041c1",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, image_transform=None, mask_transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.image_filenames = sorted(os.listdir(image_path))\n",
    "        self.mask_filenames = sorted(os.listdir(mask_path))\n",
    "        self.image_transform =image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_dir = os.path.join(self.image_path, self.image_filenames[index])\n",
    "        mask_dir = os.path.join(self.mask_path, self.mask_filenames[index])\n",
    "\n",
    "        image = Image.open(image_dir).convert('RGB')\n",
    "        mask = Image.open(mask_dir).convert('L')\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        # Removes the extra '1' channel dimension (changes shape from [1, H, W] to [H, W])\n",
    "        mask = mask.squeeze(0)\n",
    "        # Ensures all pixels are strictly 0 or 1, cleaning up any blurry gray edges caused by transforms\n",
    "        mask = torch.where(mask > 0, 1, 0).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ca9e7",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3576c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    return image_transform, mask_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6289994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(image_path, mask_path):\n",
    "    image_transform, mask_transform = get_transforms()\n",
    "\n",
    "    dataset = LungDataset(\n",
    "        image_path, \n",
    "        mask_path, \n",
    "        image_transform=image_transform, \n",
    "        mask_transform=mask_transform\n",
    "    )\n",
    "\n",
    "    # Split dataset into train/val\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_loader,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_setup(model):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74176f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    correct_pixels = 0.0\n",
    "    total_pixels = 0.0\n",
    "\n",
    "    for images, masks in loader:\n",
    "        # masks shape becomes [Batch, 1, H, W] to match model output\n",
    "        images, masks = images.to(device), masks.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, masks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Convert model outputs to binary predictions (0 or 1)\n",
    "        preds = (torch.sigmoid(output) > 0.5).float()\n",
    "\n",
    "        # This creates a boolean tensor where True represents a correct pixel match.\n",
    "        correct_pixels += (preds == masks).sum().item()\n",
    "\n",
    "        # Add the total number of pixels in this batch to our running total\n",
    "        total_pixels += torch.numel(preds)\n",
    "    \n",
    "    epoch_loss /= len(loader)\n",
    "    epoch_acc = correct_pixels / total_pixels\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f2397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

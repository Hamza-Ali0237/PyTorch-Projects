{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4d04e0",
   "metadata": {},
   "source": [
    "# Drone Images Segmentation Using SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96eb1eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d84280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (26.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.10.0+cu128)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.3)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.24.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->torchmetrics) (1.3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc3a3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.segmentation import DiceScore, MeanIoU\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95050f83",
   "metadata": {},
   "source": [
    "# Reproducibilty Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f93a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Computation Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc07144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ff31b",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23a73404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneImagesSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, filenames, joint_transforms=None, image_transforms=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.filenames = filenames\n",
    "        self.joint_transforms = joint_transforms\n",
    "        self.image_transforms = image_transforms\n",
    "\n",
    "        # Lookup table for the classes present in the mask\n",
    "        original_classes = [0, 1, 2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 19, 20, 21, 22]\n",
    "        self.lookup_table = np.zeros(23, dtype=np.int64)\n",
    "        for new_index, orig_val in enumerate(original_classes):\n",
    "            self.lookup_table[orig_val] = new_index\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_filename, mask_filename = self.filenames[index]\n",
    "        \n",
    "        image_path = os.path.join(self.images_dir, img_filename)\n",
    "        mask_path = os.path.join(self.masks_dir, mask_filename)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Remap the mask pixel values before creating the tensor\n",
    "        mask_array = np.array(mask)\n",
    "        remapped_masked = self.lookup_table[mask_array]\n",
    "\n",
    "        image = tv_tensors.Image(image)\n",
    "        mask = tv_tensors.Mask(remapped_masked)\n",
    "\n",
    "        if self.joint_transforms:\n",
    "            image, mask = self.joint_transforms(image, mask)\n",
    "        \n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "        \n",
    "        mask = mask.to(torch.long)\n",
    "\n",
    "        # Remove channel dimension for Cross Entropy\n",
    "        if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "            mask = mask.squeeze(0)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b908cd",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a21d50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(is_train=True):\n",
    "    \n",
    "    if is_train: \n",
    "        # Joint Spatial Transforms For Both Images & Masks\n",
    "        joint_transforms = v2.Compose([\n",
    "            v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "            \n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomVerticalFlip(p=0.5),\n",
    "\n",
    "            v2.RandomRotation(\n",
    "                degrees=45,\n",
    "                interpolation=InterpolationMode.NEAREST\n",
    "            ),\n",
    "\n",
    "            v2.RandomResizedCrop(\n",
    "                size=(256, 256),\n",
    "                scale=(0.8, 1.0),\n",
    "                interpolation=InterpolationMode.NEAREST\n",
    "            ),\n",
    "            \n",
    "            v2.ToImage(), # Image to tensor (Modern ToTensor() Alternative)\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "        # Image Only Transforms\n",
    "        image_transforms = v2.Compose([\n",
    "            v2.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2\n",
    "            ),\n",
    "            v2.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        joint_transforms = v2.Compose([\n",
    "            v2.Resize((256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "\n",
    "        image_transforms = v2.Compose([\n",
    "            v2.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    return joint_transforms, image_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff95c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(images_dir, masks_dir, batch_size=8):\n",
    "    \n",
    "    # Initialize Transforms\n",
    "    train_joint_transforms, train_image_transforms = get_transforms(is_train=True)\n",
    "    val_test_joint_transforms, val_test_image_transforms = get_transforms(is_train=False)\n",
    "\n",
    "\n",
    "    # --- Note to future me: ONLY use this logic when both images and masks are identical in name ---\n",
    "    # # Ensure only matching image/mask pairs are used\n",
    "    # all_images = set(os.listdir(images_dir))\n",
    "    # all_masks = set(os.listdir(masks_dir))\n",
    "    # paired_filenames = sorted(list(all_images & all_masks))\n",
    "\n",
    "    # --- New Logic ---\n",
    "    paired_filenames = []\n",
    "    for img_name in sorted(os.listdir(images_dir)):\n",
    "        if img_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        # Get the filename without the extension\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        mask_name = base_name + \".png\"\n",
    "\n",
    "        if os.path.exists(os.path.join(masks_dir, mask_name)):\n",
    "            paired_filenames.append((img_name, mask_name))\n",
    "        \n",
    "    print(f\"Found {len(paired_filenames)} valid image-mask pairs!\")\n",
    "\n",
    "    if len(paired_filenames) < 10:\n",
    "        raise ValueError(\"Not enough paired files found! Check your folder paths and file extensions.\")\n",
    "        \n",
    "\n",
    "    # Split filenames before creating datasets\n",
    "    train_files, val_test_files = train_test_split(\n",
    "        paired_filenames, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    val_files, test_files = train_test_split(\n",
    "        val_test_files, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = DroneImagesSegmentationDataset(\n",
    "        images_dir, masks_dir, train_files, train_joint_transforms, train_image_transforms\n",
    "    )\n",
    "\n",
    "    val_dataset = DroneImagesSegmentationDataset(\n",
    "        images_dir, masks_dir, val_files, val_test_joint_transforms, val_test_image_transforms\n",
    "    )\n",
    "\n",
    "    test_dataset = DroneImagesSegmentationDataset(\n",
    "        images_dir, masks_dir, test_files, val_test_joint_transforms, val_test_image_transforms\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "879f5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(num_classes, device):\n",
    "    metrics = torchmetrics.MetricCollection({\n",
    "        \"dice\": DiceScore(\n",
    "            num_classes=num_classes, include_background=True, average=None, input_format=\"index\"\n",
    "        ),\n",
    "        \"miou\": MeanIoU(\n",
    "            num_classes=num_classes, include_background=True, per_class=True, input_format=\"index\"\n",
    "        ),\n",
    "        \"pixel_acc\": torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=num_classes\n",
    "        ),\n",
    "        \"conf_mat\": torchmetrics.ConfusionMatrix(\n",
    "            task=\"multiclass\", num_classes=num_classes, normalize=\"true\" # Normalizes over true labels\n",
    "        )\n",
    "    }).to(device)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7f7c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(self, ce_weight=1.0, dice_weight=1.0, epsilon=1e-6):\n",
    "        super(DiceCELoss, self).__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.epsilon = epsilon\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute CE Loss (Numerically stable)\n",
    "        ce = self.ce_loss(inputs, targets)\n",
    "\n",
    "        # Apply Softmax to raw predictions\n",
    "        inputs = torch.softmax(inputs, dim=1)\n",
    "\n",
    "        # One-Hot encode the targets\n",
    "        targets = F.one_hot(targets, num_classes=inputs.shape[1])\n",
    "        targets = targets.permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Flatten the tensors\n",
    "        inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n",
    "        targets = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "\n",
    "        # Compute Dice Loss\n",
    "        intersection = (inputs * targets).sum(dim=2)\n",
    "        union = inputs.sum() + targets.sum(dim=2)\n",
    "\n",
    "        dice_score = (2.0 * intersection + self.epsilon) / (union + self.epsilon)\n",
    "        dice_loss = 1 - dice_score.mean()\n",
    "\n",
    "        dice_ce_loss = (ce * self.ce_weight) + (dice_loss * self.dice_weight)\n",
    "\n",
    "        return dice_ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4721bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_setup(model, epochs):\n",
    "    criterion = DiceCELoss()\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), lr=3e-4, weight_decay=1e-4, betas=(0.9, 0.999), eps=1e-8\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    return criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fd7464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, metrics, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    metrics.reset()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        metrics.update(preds, masks)\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    return epoch_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66a425c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, metrics, criterion, device):\n",
    "    model.eval()\n",
    "    metrics.reset()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            metrics.update(preds, masks)\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    return epoch_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0206c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 16 distinct RGB colors (one for each class)\n",
    "SEG_COLORS = np.array([\n",
    "    [0, 0, 0],         # 0: Background (Black)\n",
    "    [128, 0, 0],       # 1: Maroon\n",
    "    [0, 128, 0],       # 2: Green\n",
    "    [128, 128, 0],     # 3: Olive\n",
    "    [0, 0, 128],       # 4: Navy\n",
    "    [128, 0, 128],     # 5: Purple\n",
    "    [0, 128, 128],     # 6: Teal\n",
    "    [128, 128, 128],   # 7: Gray\n",
    "    [64, 0, 0],        # 8: Dark Red\n",
    "    [192, 0, 0],       # 9: Bright Red\n",
    "    [64, 128, 0],      # 10: Light Green\n",
    "    [192, 128, 0],     # 11: Orange\n",
    "    [64, 0, 128],      # 12: Deep Purple\n",
    "    [192, 0, 128],     # 13: Pink\n",
    "    [64, 128, 128],    # 14: Light Teal\n",
    "    [255, 255, 255]    # 15: White\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def decode_segmap(mask_tensor):\n",
    "    mask_np = mask_tensor.cpu().numpy()\n",
    "    # Use advanced indexing to map class integers to RGB colors instantly\n",
    "    rgb_mask = SEG_COLORS[mask_np]\n",
    "    return rgb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "51e84f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_history, val_history):\n",
    "    epochs = range(1, len(train_history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    axs[0].plot(epochs, train_history['train_loss'], label='Train Loss', color='blue')\n",
    "    axs[0].plot(epochs, val_history['val_loss'], label='Val Loss', color='orange')\n",
    "    axs[0].set_title('Cross Entropy + Dice Loss')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot mIoU (assuming you track this instead of top_1_acc)\n",
    "    axs[1].plot(epochs, train_history['train_miou'], label='Train mIoU', color='green')\n",
    "    axs[1].plot(epochs, val_history['val_miou'], label='Val mIoU', color='red')\n",
    "    axs[1].set_title('Mean Intersection over Union (mIoU)')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('mIoU Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c579bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix_tensor, class_names=None):\n",
    "    \n",
    "    # Convert the PyTorch tensor to a NumPy array\n",
    "    cm_np = conf_matrix_tensor.cpu().numpy()\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(cm_np.shape[0])]\n",
    "        \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Create the heatmap\n",
    "    # annot=True puts the numbers inside the boxes. fmt=\".2f\" formats them to 2 decimal places.\n",
    "    sns.heatmap(cm_np, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.title(\"Normalized Confusion Matrix (Final Validation Epoch)\")\n",
    "    plt.ylabel(\"Actual Class (Ground Truth)\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    \n",
    "    # Rotate the x-axis labels so they don't overlap\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f50482b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qualitative_results(model, dataloader, device, num_images=3):\n",
    "    model.eval()\n",
    "    \n",
    "    # Grab one batch of data\n",
    "    images, masks = next(iter(dataloader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Move tensors to CPU for plotting\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "    preds = preds.cpu()\n",
    "    \n",
    "    fig, axs = plt.subplots(num_images, 3, figsize=(12, 4 * num_images))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # 1. Plot Original Image (un-normalize if you used transforms like Normalize)\n",
    "        img_np = images[i].permute(1, 2, 0).numpy()\n",
    "        # Clip values to [0, 1] in case of slight normalization artifacts\n",
    "        img_np = np.clip(img_np, 0, 1) \n",
    "        axs[i, 0].imshow(img_np)\n",
    "        axs[i, 0].set_title(\"Original RGB Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        # 2. Plot Ground Truth Mask\n",
    "        gt_rgb = decode_segmap(masks[i])\n",
    "        axs[i, 1].imshow(gt_rgb)\n",
    "        axs[i, 1].set_title(\"Ground Truth Mask\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        # 3. Plot Predicted Mask\n",
    "        pred_rgb = decode_segmap(preds[i])\n",
    "        axs[i, 2].imshow(pred_rgb)\n",
    "        axs[i, 2].set_title(\"Model Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a8bc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_iou(iou_array, class_names=None):\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(len(iou_array))]\n",
    "        \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=class_names, y=iou_array, palette=\"viridis\")\n",
    "    plt.title(\"Validation IoU per Class (Final Epoch)\")\n",
    "    plt.ylabel(\"IoU Score\")\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07059fec",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "836639bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_conv_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_block = nn.ModuleList()\n",
    "\n",
    "        current_in_channels = in_channels\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            \n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(current_in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "            self.enc_block.append(layer)\n",
    "\n",
    "            current_in_channels = out_channels\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.enc_block:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x, indices = self.max_pool(x)\n",
    "        return x, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "016b5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_conv_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.max_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.dec_block = nn.ModuleList()\n",
    "\n",
    "        current_in_channels = in_channels\n",
    "\n",
    "        for i in range(num_conv_layers):\n",
    "            \n",
    "            if i+1 == num_conv_layers:\n",
    "                current_out_channels = out_channels\n",
    "            else:\n",
    "                current_out_channels = in_channels\n",
    "\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(current_in_channels, current_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(current_out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "            self.dec_block.append(layer)\n",
    "\n",
    "            current_in_channels = current_out_channels\n",
    "\n",
    "    \n",
    "    def forward(self, x, indices):\n",
    "        x = self.max_unpool(x, indices)\n",
    "\n",
    "        for layer in self.dec_block:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d081d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        # Number of Conv2d layers per encoder block \n",
    "        num_conv_layer_enc = [2, 2, 3, 3, 3]\n",
    "\n",
    "        self.enc1 = Encoder(3, 64, num_conv_layer_enc[0])\n",
    "        self.enc2 = Encoder(64, 128, num_conv_layer_enc[1])\n",
    "        self.enc3 = Encoder(128, 256, num_conv_layer_enc[2])\n",
    "        self.enc4 = Encoder(256, 512, num_conv_layer_enc[3])\n",
    "        self.enc5 = Encoder(512, 512, num_conv_layer_enc[4])\n",
    "\n",
    "\n",
    "        # Number of Conv2d layers per decoder block \n",
    "        num_conv_layer_dec = [3, 3, 3, 2, 2]\n",
    "\n",
    "        self.dec5 = Decoder(512, 512, num_conv_layer_dec[0])\n",
    "        self.dec4 = Decoder(512, 256, num_conv_layer_dec[1])\n",
    "        self.dec3 = Decoder(256, 128, num_conv_layer_dec[2])\n",
    "        self.dec2 = Decoder(128, 64, num_conv_layer_dec[3])\n",
    "        self.dec1 = Decoder(64, num_classes, num_conv_layer_dec[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, ind1 = self.enc1(x)\n",
    "        x, ind2 = self.enc2(x)\n",
    "        x, ind3 = self.enc3(x)\n",
    "        x, ind4 = self.enc4(x)\n",
    "        x, ind5 = self.enc5(x)\n",
    "\n",
    "        x = self.dec5(x, ind5)\n",
    "        x = self.dec4(x, ind4)\n",
    "        x = self.dec3(x, ind3)\n",
    "        x = self.dec2(x, ind2)\n",
    "        x = self.dec1(x, ind1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c923a2e",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "514e0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 375 valid image-mask pairs!\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "model = SegNet(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "images_dir = \"/content/drive/MyDrive/drone_dataset/images\"\n",
    "masks_dir = \"/content/drive/MyDrive/drone_dataset/masks\"\n",
    "train_loader, val_loader, test_loader = get_loaders(images_dir, masks_dir)\n",
    "\n",
    "criterion, optimizer, scheduler = training_setup(model, EPOCHS)\n",
    "train_metrics = get_metrics(NUM_CLASSES, device)\n",
    "val_metrics = get_metrics(NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1219171",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = {\"train_loss\":[], \"train_pixel_acc\":[], \"train_dice\":[], \"train_miou\":[]}\n",
    "val_history = {\"val_loss\":[], \"val_pixel_acc\":[], \"val_dice\":[], \"val_miou\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "393b5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# images_dir = \"/content/drive/MyDrive/drone_dataset/images\"\n",
    "# masks_dir = \"/content/drive/MyDrive/drone_dataset/masks\"\n",
    "\n",
    "# print(\"Scanning for corrupted files...\")\n",
    "\n",
    "# for folder in [images_dir, masks_dir]:\n",
    "#     for filename in os.listdir(folder):\n",
    "#         filepath = os.path.join(folder, filename)\n",
    "#         if filepath.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#             try:\n",
    "#                 img = Image.open(filepath)\n",
    "#                 img.verify()  # Verifies that it is, in fact, an image\n",
    "                \n",
    "#                 # verify() doesn't catch truncation, so we force it to load the data\n",
    "#                 img = Image.open(filepath) \n",
    "#                 img.load() \n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"CORRUPTED FILE FOUND: {filepath}\")\n",
    "#                 print(f\"Error: {e}\")\n",
    "                \n",
    "# print(\"Scan complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffcbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 1/50 | Train Loss: 3.6518 | Val Loss: 3.9565\n",
      "Epoch: 2/50 | Train Loss: 3.2861 | Val Loss: 3.2746\n",
      "Epoch: 3/50 | Train Loss: 3.1312 | Val Loss: 3.3233\n",
      "Epoch: 4/50 | Train Loss: 3.0560 | Val Loss: 3.0800\n",
      "Epoch: 5/50 | Train Loss: 3.0127 | Val Loss: 3.1049\n",
      "Epoch: 6/50 | Train Loss: 3.0141 | Val Loss: 3.3694\n",
      "Epoch: 7/50 | Train Loss: 3.0097 | Val Loss: 3.0549\n",
      "Epoch: 8/50 | Train Loss: 2.9772 | Val Loss: 3.0253\n",
      "Epoch: 9/50 | Train Loss: 2.9358 | Val Loss: 3.0690\n",
      "Epoch: 10/50 | Train Loss: 2.9440 | Val Loss: 3.0147\n",
      "Epoch: 11/50 | Train Loss: 2.9471 | Val Loss: 3.0109\n",
      "Epoch: 12/50 | Train Loss: 2.9555 | Val Loss: 2.9971\n",
      "Epoch: 13/50 | Train Loss: 2.8923 | Val Loss: 2.8556\n",
      "Epoch: 14/50 | Train Loss: 2.8729 | Val Loss: 2.9747\n",
      "Epoch: 15/50 | Train Loss: 2.8814 | Val Loss: 2.9455\n",
      "Epoch: 16/50 | Train Loss: 2.8395 | Val Loss: 3.0118\n",
      "Epoch: 17/50 | Train Loss: 2.8697 | Val Loss: 2.9433\n",
      "Epoch: 18/50 | Train Loss: 2.8000 | Val Loss: 2.8411\n",
      "Epoch: 19/50 | Train Loss: 2.8229 | Val Loss: 2.8835\n",
      "Epoch: 20/50 | Train Loss: 2.8078 | Val Loss: 2.8586\n",
      "Epoch: 21/50 | Train Loss: 2.7871 | Val Loss: 2.8306\n",
      "Epoch: 22/50 | Train Loss: 2.8077 | Val Loss: 2.9235\n",
      "Epoch: 23/50 | Train Loss: 2.7868 | Val Loss: 2.8271\n",
      "Epoch: 24/50 | Train Loss: 2.7495 | Val Loss: 2.8327\n",
      "Epoch: 25/50 | Train Loss: 2.7370 | Val Loss: 2.7904\n",
      "Epoch: 26/50 | Train Loss: 2.6966 | Val Loss: 2.8270\n",
      "Epoch: 27/50 | Train Loss: 2.7053 | Val Loss: 2.7966\n",
      "Epoch: 28/50 | Train Loss: 2.6947 | Val Loss: 2.7443\n",
      "Epoch: 29/50 | Train Loss: 2.6763 | Val Loss: 2.7500\n",
      "Epoch: 30/50 | Train Loss: 2.6679 | Val Loss: 2.8085\n",
      "Epoch: 31/50 | Train Loss: 2.6492 | Val Loss: 2.7135\n",
      "Epoch: 32/50 | Train Loss: 2.6493 | Val Loss: 2.6868\n",
      "Epoch: 33/50 | Train Loss: 2.6690 | Val Loss: 2.6880\n",
      "Epoch: 34/50 | Train Loss: 2.6412 | Val Loss: 2.6660\n",
      "Epoch: 35/50 | Train Loss: 2.6139 | Val Loss: 2.6538\n",
      "Epoch: 36/50 | Train Loss: 2.6336 | Val Loss: 2.6343\n",
      "Epoch: 37/50 | Train Loss: 2.6146 | Val Loss: 2.6791\n",
      "Epoch: 38/50 | Train Loss: 2.6362 | Val Loss: 2.6561\n",
      "Epoch: 39/50 | Train Loss: 2.5889 | Val Loss: 2.6357\n",
      "Epoch: 40/50 | Train Loss: 2.5934 | Val Loss: 2.6642\n",
      "Epoch: 41/50 | Train Loss: 2.5781 | Val Loss: 2.6460\n",
      "Epoch: 42/50 | Train Loss: 2.5841 | Val Loss: 2.6464\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, _ = train_one_epoch(model, train_loader, train_metrics, criterion, optimizer, device)\n",
    "    val_loss, _ = validate(model, val_loader, val_metrics, criterion, device)\n",
    "\n",
    "    train_results = train_metrics.compute()\n",
    "    val_results = val_metrics.compute()\n",
    "    \n",
    "    train_history[\"train_loss\"].append(train_loss)\n",
    "    train_history[\"train_pixel_acc\"].append(train_results['pixel_acc'].item())\n",
    "    train_history[\"train_dice\"].append(train_results['dice'].mean().item())\n",
    "    train_history['train_miou'].append(train_results['miou'].mean().item())\n",
    "\n",
    "    val_history[\"val_loss\"].append(val_loss)\n",
    "    val_history[\"val_pixel_acc\"].append(val_results['pixel_acc'].item())\n",
    "    val_history[\"val_dice\"].append(val_results['dice'].mean().item())\n",
    "    val_history['val_miou'].append(val_results['miou'].mean().item())\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(f\"Total Training Time: {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(train_history, val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b6074",
   "metadata": {},
   "source": [
    "## Evaluation On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = get_metrics(NUM_CLASSES, device)\n",
    "\n",
    "test_loss, _ = validate(model, test_loader, test_metrics, criterion, device)\n",
    "test_results = test_metrics.compute()\n",
    "\n",
    "test_pix_acc = test_results['pixel_acc'].item()\n",
    "test_dice = test_results['dice'].mean().item()\n",
    "test_miou = test_results['miou'].mean().item()\n",
    "\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Test Pixel Acc: {test_pix_acc:.2%}\")\n",
    "print(f\"Test Mean Dice: {test_dice:.4f}\")\n",
    "print(f\"Test mIoU:      {test_miou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867406f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_results['conf_mat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_class_iou(test_results['miou'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qualitative_results(model, test_loader, device, num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

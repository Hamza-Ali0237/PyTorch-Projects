{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Ali0237/PyTorch-Projects/blob/main/Intermediate/PyTorch-CelebA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f694dd",
      "metadata": {
        "id": "d4f694dd"
      },
      "source": [
        "# Multi-Label Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Configuration"
      ],
      "metadata": {
        "id": "ca7PScJTpwOR"
      },
      "id": "ca7PScJTpwOR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eccd8d9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "3eccd8d9"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be6df1d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8be6df1d"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "import torchmetrics as tm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662e44af",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "662e44af"
      },
      "outputs": [],
      "source": [
        "# Configuration Dictionary\n",
        "cfg = {\n",
        "    # Reproducibility\n",
        "    \"seed\": 42,\n",
        "\n",
        "    # Computing Device\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    # Data Settings\n",
        "    \"root_dir\": \"./data\",\n",
        "    \"image_size\": 224,\n",
        "    \"batch_size\": 32,\n",
        "    \"num_workers\": 2,\n",
        "    \"pin_memory\": True,\n",
        "    \"num_classes\": 40,\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    \"epochs\": 25,\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"label_smoothing\": 0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462fea46",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "462fea46"
      },
      "outputs": [],
      "source": [
        "device = cfg[\"device\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e340f11",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6e340f11"
      },
      "outputs": [],
      "source": [
        "# Set Seed For Reproducibilty\n",
        "SEED = cfg[\"seed\"]\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.cuda.manua nl_seed_all(SEED)\n",
        "\n",
        "# Ensure Deterministic Behaviour\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform & Load Data"
      ],
      "metadata": {
        "id": "UVJSUeIGzr-h"
      },
      "id": "UVJSUeIGzr-h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6873d4a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b6873d4a"
      },
      "outputs": [],
      "source": [
        "# Normalization Values\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "image_size = cfg[\"image_size\"]\n",
        "\n",
        "# Training Transforms (Augmentation+ Normalization)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(\n",
        "        brightness = 0.1, contrast = 0.1, saturation = 0.1, hue = 0.05\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean = mean, std = std\n",
        "    )\n",
        "])\n",
        "\n",
        "# Validation & Test Transforms\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean = mean, std = std\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b82a5e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "29b82a5e"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "root = cfg[\"root_dir\"]\n",
        "\n",
        "train_dataset = CelebA(\n",
        "    root = root, split = \"train\",\n",
        "    transform = train_transforms, download = True\n",
        ")\n",
        "\n",
        "val_dataset = CelebA(\n",
        "    root = root, split = \"val\",\n",
        "    transform = val_test_transforms, download = True\n",
        ")\n",
        "\n",
        "test_dataset = CelebA(\n",
        "    root = root, split = \"test\",\n",
        "    transform = val_test_transforms, download = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527b8b7b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "527b8b7b"
      },
      "outputs": [],
      "source": [
        "# Define DataLoaders\n",
        "batch_size = cfg[\"batch_size\"]\n",
        "num_workers = cfg[\"num_workers\"]\n",
        "pin_memory = cfg[\"pin_memory\"]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size = batch_size,\n",
        "    shuffle = True, num_workers = num_workers,\n",
        "    pin_memory = pin_memory\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size = batch_size,\n",
        "    shuffle = False, num_workers = num_workers,\n",
        "    pin_memory = pin_memory\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size = batch_size,\n",
        "    shuffle = False, num_workers = num_workers,\n",
        "    pin_memory = pin_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Class"
      ],
      "metadata": {
        "id": "h7ePDB9HhC8F"
      },
      "id": "h7ePDB9HhC8F"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelCNN(nn.Module):\n",
        "  def __init__(self, num_classes, image_size):\n",
        "        super(MultiLabelClass, self).__init__()\n",
        "\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size = 3, stride = 1, padding = 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "\n",
        "        nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    # Dynamically Calculate Flattened Size\n",
        "    with torch.no_grad():\n",
        "      dummy = torch.zeros(1, 3, image_size, image_size)\n",
        "      feat_dim = self.features(dummy).view(1, -1).shape[1]\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        self.Flatten(),\n",
        "        self.Linear(feat_dim, 512),\n",
        "        self.ReLU(),\n",
        "        self.Dropout(0.5),\n",
        "        self.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "M_Gpr2h7gq1l"
      },
      "id": "M_Gpr2h7gq1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation Functions"
      ],
      "metadata": {
        "id": "zXg8nx9Si-ys"
      },
      "id": "zXg8nx9Si-ys"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Evaluation Metrics\n",
        "\n",
        "# Accuracy\n",
        "accuracy = tm.MultilabelAccuracy(\n",
        "    num_labels = num_classes,\n",
        "    threshold = 0.5\n",
        ").to(device)\n",
        "\n",
        "# F1 Score\n",
        "f1_macro = tm.MultilabelF1(\n",
        "    num_labels = num_classes,\n",
        "    average = \"macro\",\n",
        "    threshold = 0.5\n",
        ").to(device)\n",
        "\n",
        "f1_micro = tm.MultilabelF1(\n",
        "    num_labels = num_classes,\n",
        "    average = \"micro\",\n",
        "    threshold = 0.5\n",
        ").to(device)\n",
        "\n",
        "# Area Under ROC Curve (AUROC)\n",
        "auroc_macro = tm.MultilabelAUROC(\n",
        "    num_labels = num_classes,\n",
        "    average = \"macro\"\n",
        ").to(device)\n",
        "\n",
        "# Mean Average Precision (mAP)\n",
        "map_macro = tm.MultilabelAveragePrecision(\n",
        "    num_labels=num_classes,\n",
        "    average=\"macro\"\n",
        ").to(device)\n",
        "\n",
        "map_micro = tm.MultilabelAveragePrecision(\n",
        "    num_labels=num_classes,\n",
        "    average=\"micro\"\n",
        ").to(device)\n",
        "\n",
        "metrics_dict = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"f1_macro\": f1_macro,\n",
        "    \"f1_micro\": f1_micro,\n",
        "    \"auroc_macro\": auroc_macro,\n",
        "    \"map_macro\": map_macro,\n",
        "    \"map_micro\": map_micro\n",
        "}"
      ],
      "metadata": {
        "id": "o6Sac7P7gqtf"
      },
      "id": "o6Sac7P7gqtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training & Validation Function\n",
        "def train_vaL_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, metrics_dict):\n",
        "\n",
        "  # Store History\n",
        "  history = {\n",
        "      \"train_loss\": [], \"val_loss\": [],\n",
        "      \"train_acc\": [], \"val_acc\": [],\n",
        "      \"val_f1_macro\": [], \"val_f1_micro\": [],\n",
        "      \"val_auroc_macro\": [], \"val_map_macro\": [], \"val_map_micro\": []\n",
        "  }\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimzer.step()\n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_train_loss += loss.item()\n",
        "\n",
        "      preds = torch.sigmoid(outputs)\n",
        "      metrics_dict[\"accuracy\"](preds, labels)\n",
        "\n",
        "    epoch_train_loss /= len(train_loader)\n",
        "    epoch_train_acc = metrics_dict[\"accuracy\"].compute().item()\n",
        "    metrics_dict[\"accuracy\"].reset()\n",
        "\n",
        "    history[\"train_loss\"].append(epoch_train_loss)\n",
        "    history[\"train_acc\"].append(epoch_train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        epoch_val_loss += loss.item()\n",
        "\n",
        "        preds = torch.sigmoid(outputs)\n",
        "\n",
        "        # Update All Validation Metrics\n",
        "        metrics_dict[\"accuracy\"](preds, labels)\n",
        "        metrics_dict[\"f1_macro\"](preds, labels)\n",
        "        metrics_dict[\"f1_micro\"](preds, labels)\n",
        "        metrics_dict[\"auroc_macro\"](preds, labels)\n",
        "        metrics_dict[\"map_macro\"](preds, labels)\n",
        "        metrics_dict[\"map_micro\"](preds, labels)\n",
        "\n",
        "    epoch_val_loss /= len(val_loader)\n",
        "    epoch_val_acc = metrics_dict[\"accuracy\"].compute().item()\n",
        "    epoch_val_f1_macro = metrics_dict[\"f1_macro\"].compute().item()\n",
        "    epoch_val_f1_micro = metrics_dict[\"f1_micro\"].compute().item()\n",
        "    epoch_val_auroc_macro = metrics_dict[\"auroc_macro\"].compute().item()\n",
        "    epoch_val_map_macro = metrics_dict[\"map_macro\"].compute().item()\n",
        "    epoch_val_map_micro = metrics_dict[\"map_micro\"].compute().item()\n",
        "\n",
        "    # Reset Metrics\n",
        "    for metric in metrics_dict.value():\n",
        "      metric.reset()\n",
        "\n",
        "    # Save History\n",
        "    history[\"val_loss\"].append(epoch_val_loss)\n",
        "    history[\"val_acc\"].append(epoch_val_acc)\n",
        "    history[\"val_f1_macro\"].append(epoch_val_f1_macro)\n",
        "    history[\"val_f1_micro\"].append(epoch_val_f1_micro)\n",
        "    history[\"val_auroc_macro\"].append(epoch_val_auroc_micro)\n",
        "    history[\"val_map_macro\"].append(epoch_val_map_macro)\n",
        "    history[\"val_map_micro\"].append(epoch_val_map_micro)\n",
        "\n",
        "    # Print progress\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f} | \"\n",
        "        f\"Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}, \"\n",
        "        f\"F1_macro: {epoch_val_f1_macro:.4f}, F1_micro: {epoch_val_f1_micro:.4f}, \"\n",
        "        f\"AUROC_macro: {epoch_val_auroc_macro:.4f}, \"\n",
        "        f\"mAP_macro: {epoch_val_map_macro:.4f}, mAP_micro: {epoch_val_map_micro:.4f}\"\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "A_jbknErgqqY"
      },
      "id": "A_jbknErgqqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lS_Sx04gqni"
      },
      "id": "1lS_Sx04gqni",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}